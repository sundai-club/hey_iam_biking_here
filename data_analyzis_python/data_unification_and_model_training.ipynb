{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cambridge Bikes Data Unification and Model Training\n",
        "\n",
        "This notebook will:\n",
        "1. Clean and unify data from three sources:\n",
        "   - Bicycle crashes data (cleaned)\n",
        "   - City bike count data\n",
        "   - Eco-totem automatic counter data\n",
        "2. Create features for TensorFlow model training\n",
        "3. Train models to predict bicycle counts and accident severity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the three datasets\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# 1. Bicycle crashes data (already cleaned)\n",
        "crashes_df = pd.read_csv('data/processed/bicycle_crashes_cleaned.csv')\n",
        "print(f\"Crashes data shape: {crashes_df.shape}\")\n",
        "\n",
        "# 2. City bike count data\n",
        "city_count_df = pd.read_csv('data/city_bike_count.csv')\n",
        "print(f\"City count data shape: {city_count_df.shape}\")\n",
        "\n",
        "# 3. Eco-totem data\n",
        "eco_totem_df = pd.read_csv('data/eco_totem.csv')\n",
        "print(f\"Eco-totem data shape: {eco_totem_df.shape}\")\n",
        "\n",
        "print(\"\\nDatasets loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the structure of each dataset\n",
        "print(\"=== CRASHES DATA ===\")\n",
        "print(crashes_df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "print(crashes_df.head(2))\n",
        "\n",
        "print(\"\\n=== CITY COUNT DATA ===\")\n",
        "print(city_count_df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "print(city_count_df.head(2))\n",
        "\n",
        "print(\"\\n=== ECO-TOTEM DATA ===\")\n",
        "print(eco_totem_df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "print(eco_totem_df.head(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine intersection names in crashes data\n",
        "print(\"=== CRASHES DATA INTERSECTIONS ===\")\n",
        "print(f\"Unique intersections: {crashes_df['Intersection_ID'].nunique()}\")\n",
        "print(\"\\nTop 10 intersections by crash count:\")\n",
        "print(crashes_df['Intersection_ID'].value_counts().head(10))\n",
        "\n",
        "print(\"\\n=== CITY COUNT DATA LOCATIONS ===\")\n",
        "print(f\"Unique count locations: {city_count_df['Count Location'].nunique()}\")\n",
        "print(\"\\nTop 10 count locations:\")\n",
        "print(city_count_df['Count Location'].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create intersection name mapping function\n",
        "def normalize_intersection_name(name):\n",
        "    \"\"\"Normalize intersection names to a common format\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return None\n",
        "    \n",
        "    # Convert to uppercase and clean\n",
        "    name = str(name).upper().strip()\n",
        "    \n",
        "    # Handle different formats\n",
        "    if '_AND_' in name:\n",
        "        # Format: \"STREET1_AND_STREET2\"\n",
        "        parts = name.split('_AND_')\n",
        "        if len(parts) == 2:\n",
        "            return f\"{parts[0].strip()} & {parts[1].strip()}\"\n",
        "    elif ' & ' in name:\n",
        "        # Format: \"Street1 & Street2\" \n",
        "        return name\n",
        "    \n",
        "    # Single street names\n",
        "    return name\n",
        "\n",
        "# Test the function\n",
        "test_names = [\n",
        "    'BEECH STREET_AND_MASSACHUSETTS AVENUE',\n",
        "    'MASSACHUSETTS AVENUE',\n",
        "    'Concord Ave & Garden St',\n",
        "    'Broadway & Hampshire St'\n",
        "]\n",
        "\n",
        "print(\"Testing intersection name normalization:\")\n",
        "for name in test_names:\n",
        "    normalized = normalize_intersection_name(name)\n",
        "    print(f\"'{name}' -> '{normalized}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create weather condition mapping function\n",
        "def normalize_weather_condition(weather):\n",
        "    \"\"\"Normalize weather conditions to standard categories\"\"\"\n",
        "    if pd.isna(weather):\n",
        "        return 'UNKNOWN'\n",
        "    \n",
        "    weather = str(weather).upper().strip()\n",
        "    \n",
        "    # Map to standard categories\n",
        "    if any(word in weather for word in ['CLEAR', 'SUNNY']):\n",
        "        return 'CLEAR'\n",
        "    elif any(word in weather for word in ['CLOUDY', 'OVERCAST', 'BROKEN CLOUDS', 'SCATTERED CLOUDS', 'FEW CLOUDS', 'PARTIALLY CLOUDY', 'PARTLY CLOUDY', 'MOSTLY CLOUDY']):\n",
        "        return 'CLOUDY'\n",
        "    elif any(word in weather for word in ['RAIN', 'DRIZZLE']):\n",
        "        return 'RAIN'\n",
        "    elif any(word in weather for word in ['SNOW', 'SLEET', 'HAIL', 'FREEZING']):\n",
        "        return 'SNOW'\n",
        "    else:\n",
        "        return 'UNKNOWN'\n",
        "\n",
        "# Test the function\n",
        "test_weather = [\n",
        "    'CLEAR',\n",
        "    'CLOUDY', \n",
        "    'RAIN',\n",
        "    'SNOW',\n",
        "    'Clear',\n",
        "    'Partially cloudy',\n",
        "    'Light rain',\n",
        "    'Overcast clouds'\n",
        "]\n",
        "\n",
        "print(\"Testing weather condition normalization:\")\n",
        "for weather in test_weather:\n",
        "    normalized = normalize_weather_condition(weather)\n",
        "    print(f\"'{weather}' -> '{normalized}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process crashes data\n",
        "print(\"=== PROCESSING CRASHES DATA ===\")\n",
        "\n",
        "# Create normalized intersection names\n",
        "crashes_df['normalized_intersection'] = crashes_df['Intersection_ID'].apply(normalize_intersection_name)\n",
        "\n",
        "# Normalize weather conditions\n",
        "crashes_df['normalized_weather'] = crashes_df['Weather Condition 1'].apply(normalize_weather_condition)\n",
        "\n",
        "# Convert datetime\n",
        "crashes_df['datetime'] = pd.to_datetime(crashes_df['Date Time'])\n",
        "\n",
        "# Create time features\n",
        "crashes_df['hour'] = crashes_df['datetime'].dt.hour\n",
        "crashes_df['minute'] = crashes_df['datetime'].dt.minute\n",
        "crashes_df['time_15min'] = (crashes_df['hour'] * 4 + crashes_df['minute'] // 15) % 96  # 0-95 for 15-min intervals\n",
        "crashes_df['day_of_week'] = crashes_df['datetime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "crashes_df['month'] = crashes_df['datetime'].dt.month\n",
        "\n",
        "# Create accident severity categories (as floats for model)\n",
        "def get_accident_severity(row):\n",
        "    \"\"\"Convert injury data to severity scores (0-1 range)\"\"\"\n",
        "    p1_injury = str(row['P1 Injury']).upper() if pd.notna(row['P1 Injury']) else ''\n",
        "    p2_injury = str(row['P2 Injury']).upper() if pd.notna(row['P2 Injury']) else ''\n",
        "    \n",
        "    # Check for severe injuries\n",
        "    if any(severity in p1_injury or severity in p2_injury for severity in ['FATAL', 'SUSPECTED SERIOUS', 'SERIOUS']):\n",
        "        return 1.0  # Severe\n",
        "    elif any(severity in p1_injury or severity in p2_injury for severity in ['SUSPECTED MINOR', 'MINOR']):\n",
        "        return 0.5  # Moderate\n",
        "    else:\n",
        "        return 0.1  # Light (no apparent injury but still an accident)\n",
        "\n",
        "crashes_df['accident_severity'] = crashes_df.apply(get_accident_severity, axis=1)\n",
        "\n",
        "print(f\"Processed crashes data: {crashes_df.shape}\")\n",
        "print(f\"Unique normalized intersections: {crashes_df['normalized_intersection'].nunique()}\")\n",
        "print(f\"Weather distribution: {crashes_df['normalized_weather'].value_counts().to_dict()}\")\n",
        "print(f\"Accident severity distribution: {crashes_df['accident_severity'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process city bike count data\n",
        "print(\"=== PROCESSING CITY BIKE COUNT DATA ===\")\n",
        "\n",
        "# Create normalized intersection names\n",
        "city_count_df['normalized_intersection'] = city_count_df['Count Location'].apply(normalize_intersection_name)\n",
        "\n",
        "# Normalize weather conditions\n",
        "city_count_df['normalized_weather'] = city_count_df['Weather'].apply(normalize_weather_condition)\n",
        "\n",
        "# Convert datetime\n",
        "city_count_df['datetime'] = pd.to_datetime(city_count_df['Date'] + ' ' + city_count_df['Time'])\n",
        "\n",
        "# Create time features\n",
        "city_count_df['hour'] = city_count_df['datetime'].dt.hour\n",
        "city_count_df['minute'] = city_count_df['datetime'].dt.minute\n",
        "city_count_df['time_15min'] = (city_count_df['hour'] * 4 + city_count_df['minute'] // 15) % 96\n",
        "city_count_df['day_of_week'] = city_count_df['datetime'].dt.dayofweek\n",
        "city_count_df['month'] = city_count_df['datetime'].dt.month\n",
        "\n",
        "# Aggregate counts by intersection and time period (sum all movement types)\n",
        "city_count_agg = city_count_df.groupby([\n",
        "    'normalized_intersection', 'datetime', 'time_15min', 'day_of_week', \n",
        "    'month', 'normalized_weather', 'Temperature'\n",
        "]).agg({\n",
        "    'Count': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "print(f\"Processed city count data: {city_count_agg.shape}\")\n",
        "print(f\"Unique normalized intersections: {city_count_agg['normalized_intersection'].nunique()}\")\n",
        "print(f\"Weather distribution: {city_count_agg['normalized_weather'].value_counts().to_dict()}\")\n",
        "print(f\"Date range: {city_count_agg['datetime'].min()} to {city_count_agg['datetime'].max()}\")\n",
        "print(f\"Average daily count: {city_count_agg['Count'].mean():.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process eco-totem data\n",
        "print(\"=== PROCESSING ECO-TOTEM DATA ===\")\n",
        "\n",
        "# Convert datetime\n",
        "eco_totem_df['datetime'] = pd.to_datetime(eco_totem_df['DateTime'])\n",
        "\n",
        "# Create time features\n",
        "eco_totem_df['hour'] = eco_totem_df['datetime'].dt.hour\n",
        "eco_totem_df['minute'] = eco_totem_df['datetime'].dt.minute\n",
        "eco_totem_df['time_15min'] = (eco_totem_df['hour'] * 4 + eco_totem_df['minute'] // 15) % 96\n",
        "eco_totem_df['day_of_week'] = eco_totem_df['datetime'].dt.dayofweek\n",
        "eco_totem_df['month'] = eco_totem_df['datetime'].dt.month\n",
        "\n",
        "# Eco-totem is at Broadway location (based on description)\n",
        "eco_totem_df['normalized_intersection'] = 'BROADWAY'\n",
        "\n",
        "# We don't have weather data for eco-totem, so we'll need to fill this later\n",
        "eco_totem_df['normalized_weather'] = 'UNKNOWN'\n",
        "\n",
        "print(f\"Processed eco-totem data: {eco_totem_df.shape}\")\n",
        "print(f\"Date range: {eco_totem_df['datetime'].min()} to {eco_totem_df['datetime'].max()}\")\n",
        "print(f\"Average daily total: {eco_totem_df['Total'].mean():.1f}\")\n",
        "print(f\"Peak hour average: {eco_totem_df.groupby('hour')['Total'].mean().max():.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified dataset for model training\n",
        "print(\"=== CREATING UNIFIED DATASET ===\")\n",
        "\n",
        "# First, let's get weather data for eco-totem by interpolating from city count data\n",
        "# We'll use the closest available weather data by date\n",
        "\n",
        "# Create a weather lookup from city count data\n",
        "weather_lookup = city_count_agg[['datetime', 'normalized_weather', 'Temperature']].drop_duplicates()\n",
        "weather_lookup['date'] = weather_lookup['datetime'].dt.date\n",
        "\n",
        "# For eco-totem data, find closest weather data\n",
        "def get_weather_for_date(target_date, weather_lookup):\n",
        "    \"\"\"Get weather data for a given date, using closest available date\"\"\"\n",
        "    target_date_only = target_date.date()\n",
        "    \n",
        "    # Find exact match first\n",
        "    exact_match = weather_lookup[weather_lookup['date'] == target_date_only]\n",
        "    if not exact_match.empty:\n",
        "        return exact_match.iloc[0]['normalized_weather'], exact_match.iloc[0]['Temperature']\n",
        "    \n",
        "    # Find closest date within 7 days\n",
        "    weather_lookup['date_diff'] = abs((weather_lookup['date'] - target_date_only).dt.days)\n",
        "    closest = weather_lookup[weather_lookup['date_diff'] <= 7].sort_values('date_diff')\n",
        "    \n",
        "    if not closest.empty:\n",
        "        return closest.iloc[0]['normalized_weather'], closest.iloc[0]['Temperature']\n",
        "    \n",
        "    # Default to clear weather if no data found\n",
        "    return 'CLEAR', 20.0\n",
        "\n",
        "# Apply weather lookup to eco-totem data\n",
        "print(\"Filling weather data for eco-totem...\")\n",
        "weather_data = []\n",
        "temp_data = []\n",
        "\n",
        "for _, row in eco_totem_df.iterrows():\n",
        "    weather, temp = get_weather_for_date(row['datetime'], weather_lookup)\n",
        "    weather_data.append(weather)\n",
        "    temp_data.append(temp)\n",
        "\n",
        "eco_totem_df['normalized_weather'] = weather_data\n",
        "eco_totem_df['Temperature'] = temp_data\n",
        "\n",
        "print(f\"Eco-totem weather distribution: {eco_totem_df['normalized_weather'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified dataset by combining all three sources\n",
        "print(\"=== COMBINING ALL DATA SOURCES ===\")\n",
        "\n",
        "# Prepare city count data for merging\n",
        "city_count_unified = city_count_agg[['normalized_intersection', 'datetime', 'time_15min', \n",
        "                                    'day_of_week', 'month', 'normalized_weather', 'Temperature', 'Count']].copy()\n",
        "city_count_unified['data_source'] = 'city_count'\n",
        "city_count_unified['accident_severity'] = 0.0  # No accidents in count data\n",
        "\n",
        "# Prepare eco-totem data for merging\n",
        "eco_totem_unified = eco_totem_df[['normalized_intersection', 'datetime', 'time_15min', \n",
        "                                 'day_of_week', 'month', 'normalized_weather', 'Temperature', 'Total']].copy()\n",
        "eco_totem_unified = eco_totem_unified.rename(columns={'Total': 'Count'})\n",
        "eco_totem_unified['data_source'] = 'eco_totem'\n",
        "eco_totem_unified['accident_severity'] = 0.0  # No accidents in count data\n",
        "\n",
        "# Prepare crashes data for merging (we need to create count data from crashes)\n",
        "# For crashes, we'll create a sparse dataset where most time periods have 0 accidents\n",
        "crashes_unified = crashes_df[['normalized_intersection', 'datetime', 'time_15min', \n",
        "                             'day_of_week', 'month', 'normalized_weather', 'accident_severity']].copy()\n",
        "crashes_unified['Count'] = 0  # Crashes don't represent bike counts\n",
        "crashes_unified['Temperature'] = 20.0  # Default temperature for crashes\n",
        "crashes_unified['data_source'] = 'crashes'\n",
        "\n",
        "# Combine all datasets\n",
        "unified_df = pd.concat([city_count_unified, eco_totem_unified, crashes_unified], ignore_index=True)\n",
        "\n",
        "print(f\"Unified dataset shape: {unified_df.shape}\")\n",
        "print(f\"Data sources: {unified_df['data_source'].value_counts().to_dict()}\")\n",
        "print(f\"Unique intersections: {unified_df['normalized_intersection'].nunique()}\")\n",
        "print(f\"Date range: {unified_df['datetime'].min()} to {unified_df['datetime'].max()}\")\n",
        "print(f\"Total bike count records: {len(unified_df[unified_df['Count'] > 0])}\")\n",
        "print(f\"Total accident records: {len(unified_df[unified_df['accident_severity'] > 0])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive time-series dataset for model training\n",
        "print(\"=== CREATING TIME-SERIES DATASET FOR MODEL TRAINING ===\")\n",
        "\n",
        "# We need to create a complete time series for each intersection\n",
        "# This will help us predict both bike counts and accident probabilities\n",
        "\n",
        "# Get all unique intersections\n",
        "all_intersections = unified_df['normalized_intersection'].unique()\n",
        "print(f\"Total unique intersections: {len(all_intersections)}\")\n",
        "\n",
        "# Create a complete time grid (15-minute intervals for a reasonable time range)\n",
        "# We'll focus on the most recent data (2015-2024) for better model performance\n",
        "start_date = pd.to_datetime('2015-01-01')\n",
        "end_date = pd.to_datetime('2024-12-31')\n",
        "\n",
        "# Create 15-minute time intervals\n",
        "time_intervals = pd.date_range(start=start_date, end=end_date, freq='15min')\n",
        "print(f\"Total time intervals: {len(time_intervals)}\")\n",
        "\n",
        "# Create a base dataset with all intersections and time intervals\n",
        "# This is computationally intensive, so we'll sample strategically\n",
        "print(\"Creating base time series...\")\n",
        "\n",
        "# Sample intersections (focus on those with most data)\n",
        "intersection_counts = unified_df['normalized_intersection'].value_counts()\n",
        "top_intersections = intersection_counts.head(10).index.tolist()  # Top 10 intersections\n",
        "print(f\"Focusing on top intersections: {top_intersections}\")\n",
        "\n",
        "# Sample time periods (focus on recent years and key months)\n",
        "recent_years = [2020, 2021, 2022, 2023, 2024]\n",
        "key_months = [3, 4, 5, 6, 7, 8, 9, 10]  # Spring to Fall\n",
        "\n",
        "# Create base dataset for top intersections\n",
        "base_data = []\n",
        "for intersection in top_intersections:\n",
        "    for year in recent_years:\n",
        "        for month in key_months:\n",
        "            # Get all 15-minute intervals for this month\n",
        "            month_start = pd.to_datetime(f'{year}-{month:02d}-01')\n",
        "            if month == 12:\n",
        "                month_end = pd.to_datetime(f'{year+1}-01-01')\n",
        "            else:\n",
        "                month_end = pd.to_datetime(f'{year}-{month+1:02d}-01')\n",
        "            \n",
        "            month_intervals = pd.date_range(start=month_start, end=month_end, freq='15min')\n",
        "            \n",
        "            for dt in month_intervals:\n",
        "                base_data.append({\n",
        "                    'normalized_intersection': intersection,\n",
        "                    'datetime': dt,\n",
        "                    'time_15min': (dt.hour * 4 + dt.minute // 15) % 96,\n",
        "                    'day_of_week': dt.dayofweek,\n",
        "                    'month': dt.month\n",
        "                })\n",
        "\n",
        "base_df = pd.DataFrame(base_data)\n",
        "print(f\"Base dataset shape: {base_df.shape}\")\n",
        "\n",
        "# Merge with actual data\n",
        "model_df = base_df.merge(\n",
        "    unified_df[['normalized_intersection', 'datetime', 'normalized_weather', 'Temperature', 'Count', 'accident_severity']], \n",
        "    on=['normalized_intersection', 'datetime'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing values\n",
        "model_df['Count'] = model_df['Count'].fillna(0)\n",
        "model_df['accident_severity'] = model_df['accident_severity'].fillna(0)\n",
        "model_df['normalized_weather'] = model_df['normalized_weather'].fillna('CLEAR')\n",
        "model_df['Temperature'] = model_df['Temperature'].fillna(20.0)\n",
        "\n",
        "print(f\"Model dataset shape: {model_df.shape}\")\n",
        "print(f\"Records with bike counts > 0: {len(model_df[model_df['Count'] > 0])}\")\n",
        "print(f\"Records with accidents > 0: {len(model_df[model_df['accident_severity'] > 0])}\")\n",
        "print(f\"Weather distribution: {model_df['normalized_weather'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for TensorFlow model training\n",
        "print(\"=== PREPARING DATA FOR TENSORFLOW MODEL ===\")\n",
        "\n",
        "# Focus on the most recent and complete data\n",
        "# Filter to recent years and top intersections\n",
        "recent_data = unified_df[unified_df['datetime'] >= '2020-01-01'].copy()\n",
        "\n",
        "# Get top intersections by data volume\n",
        "intersection_counts = recent_data['normalized_intersection'].value_counts()\n",
        "top_intersections = intersection_counts.head(8).index.tolist()  # Top 8 intersections\n",
        "print(f\"Top intersections for model: {top_intersections}\")\n",
        "\n",
        "# Filter to top intersections\n",
        "model_data = recent_data[recent_data['normalized_intersection'].isin(top_intersections)].copy()\n",
        "\n",
        "# Create features for the model\n",
        "print(\"Creating model features...\")\n",
        "\n",
        "# Encode categorical variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Intersection encoding\n",
        "intersection_encoder = LabelEncoder()\n",
        "model_data['intersection_encoded'] = intersection_encoder.fit_transform(model_data['normalized_intersection'])\n",
        "\n",
        "# Weather encoding\n",
        "weather_encoder = LabelEncoder()\n",
        "model_data['weather_encoded'] = weather_encoder.fit_transform(model_data['normalized_weather'])\n",
        "\n",
        "# Create additional time features\n",
        "model_data['hour_sin'] = np.sin(2 * np.pi * model_data['datetime'].dt.hour / 24)\n",
        "model_data['hour_cos'] = np.cos(2 * np.pi * model_data['datetime'].dt.hour / 24)\n",
        "model_data['day_sin'] = np.sin(2 * np.pi * model_data['day_of_week'] / 7)\n",
        "model_data['day_cos'] = np.cos(2 * np.pi * model_data['day_of_week'] / 7)\n",
        "model_data['month_sin'] = np.sin(2 * np.pi * model_data['month'] / 12)\n",
        "model_data['month_cos'] = np.cos(2 * np.pi * model_data['month'] / 12)\n",
        "\n",
        "# Normalize temperature\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "temp_scaler = StandardScaler()\n",
        "model_data['temperature_normalized'] = temp_scaler.fit_transform(model_data[['Temperature']])\n",
        "\n",
        "print(f\"Model data shape: {model_data.shape}\")\n",
        "print(f\"Date range: {model_data['datetime'].min()} to {model_data['datetime'].max()}\")\n",
        "print(f\"Records with bike counts > 0: {len(model_data[model_data['Count'] > 0])}\")\n",
        "print(f\"Records with accidents > 0: {len(model_data[model_data['accident_severity'] > 0])}\")\n",
        "\n",
        "# Save the processed data\n",
        "model_data.to_csv('data/processed/unified_model_data.csv', index=False)\n",
        "print(\"Saved unified model data to data/processed/unified_model_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train TensorFlow model\n",
        "print(\"=== CREATING TENSORFLOW MODEL ===\")\n",
        "\n",
        "# Prepare features and targets\n",
        "feature_columns = [\n",
        "    'time_15min', 'intersection_encoded', 'weather_encoded', \n",
        "    'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos',\n",
        "    'temperature_normalized'\n",
        "]\n",
        "\n",
        "X = model_data[feature_columns].values\n",
        "y_bike_count = model_data['Count'].values\n",
        "y_accident_severity = model_data['accident_severity'].values\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Bike count target shape: {y_bike_count.shape}\")\n",
        "print(f\"Accident severity target shape: {y_accident_severity.shape}\")\n",
        "\n",
        "# Split data for training and validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_bike_train, y_bike_test, y_accident_train, y_accident_test = train_test_split(\n",
        "    X, y_bike_count, y_accident_severity, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# Normalize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "feature_scaler = StandardScaler()\n",
        "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
        "X_test_scaled = feature_scaler.transform(X_test)\n",
        "\n",
        "# Create TensorFlow model for dual prediction\n",
        "print(\"Building TensorFlow model...\")\n",
        "\n",
        "# Input layer\n",
        "inputs = tf.keras.Input(shape=(X_train_scaled.shape[1],), name='features')\n",
        "\n",
        "# Shared hidden layers\n",
        "x = tf.keras.layers.Dense(128, activation='relu', name='hidden1')(inputs)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu', name='hidden2')(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "x = tf.keras.layers.Dense(32, activation='relu', name='hidden3')(x)\n",
        "\n",
        "# Two output heads\n",
        "bike_count_output = tf.keras.layers.Dense(1, activation='relu', name='bike_count')(x)\n",
        "accident_severity_output = tf.keras.layers.Dense(1, activation='sigmoid', name='accident_severity')(x)\n",
        "\n",
        "# Create model\n",
        "model = tf.keras.Model(\n",
        "    inputs=inputs,\n",
        "    outputs=[bike_count_output, accident_severity_output],\n",
        "    name='bicycle_prediction_model'\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'bike_count': 'mse',\n",
        "        'accident_severity': 'mse'\n",
        "    },\n",
        "    loss_weights={\n",
        "        'bike_count': 1.0,\n",
        "        'accident_severity': 10.0  # Higher weight for accident prediction\n",
        "    },\n",
        "    metrics={\n",
        "        'bike_count': ['mae'],\n",
        "        'accident_severity': ['mae']\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Model architecture:\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"=== TRAINING TENSORFLOW MODEL ===\")\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled,\n",
        "    [y_bike_train, y_accident_train],\n",
        "    validation_data=(X_test_scaled, [y_bike_test, y_accident_test]),\n",
        "    epochs=100,\n",
        "    batch_size=512,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"=== MODEL EVALUATION ===\")\n",
        "test_loss = model.evaluate(X_test_scaled, [y_bike_test, y_accident_test], verbose=0)\n",
        "print(f\"Test loss: {test_loss}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test_scaled)\n",
        "bike_predictions = predictions[0].flatten()\n",
        "accident_predictions = predictions[1].flatten()\n",
        "\n",
        "print(f\"Bike count predictions - Mean: {bike_predictions.mean():.2f}, Std: {bike_predictions.std():.2f}\")\n",
        "print(f\"Accident severity predictions - Mean: {accident_predictions.mean():.3f}, Std: {accident_predictions.std():.3f}\")\n",
        "\n",
        "# Calculate some basic metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "bike_mae = mean_absolute_error(y_bike_test, bike_predictions)\n",
        "bike_mse = mean_squared_error(y_bike_test, bike_predictions)\n",
        "bike_r2 = r2_score(y_bike_test, bike_predictions)\n",
        "\n",
        "accident_mae = mean_absolute_error(y_accident_test, accident_predictions)\n",
        "accident_mse = mean_squared_error(y_accident_test, accident_predictions)\n",
        "\n",
        "print(f\"\\\\nBike Count Prediction Metrics:\")\n",
        "print(f\"  MAE: {bike_mae:.2f}\")\n",
        "print(f\"  MSE: {bike_mse:.2f}\")\n",
        "print(f\"  R²: {bike_r2:.3f}\")\n",
        "\n",
        "print(f\"\\\\nAccident Severity Prediction Metrics:\")\n",
        "print(f\"  MAE: {accident_mae:.3f}\")\n",
        "print(f\"  MSE: {accident_mse:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model and preprocessing objects\n",
        "print(\"=== SAVING MODEL AND PREPROCESSORS ===\")\n",
        "\n",
        "# Save the TensorFlow model\n",
        "model.save('models/bicycle_prediction_model.h5')\n",
        "print(\"Saved TensorFlow model to models/bicycle_prediction_model.h5\")\n",
        "\n",
        "# Save preprocessing objects\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save encoders and scalers\n",
        "with open('models/intersection_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(intersection_encoder, f)\n",
        "\n",
        "with open('models/weather_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(weather_encoder, f)\n",
        "\n",
        "with open('models/temp_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(temp_scaler, f)\n",
        "\n",
        "with open('models/feature_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(feature_scaler, f)\n",
        "\n",
        "print(\"Saved preprocessing objects to models/ directory\")\n",
        "\n",
        "# Create a prediction function for easy use\n",
        "def predict_bicycle_traffic(intersection, time_15min, weather, temperature, day_of_week, month):\n",
        "    \"\"\"\n",
        "    Predict bicycle count and accident severity for given conditions\n",
        "    \n",
        "    Args:\n",
        "        intersection: Intersection name (string)\n",
        "        time_15min: Time in 15-minute increments (0-95)\n",
        "        weather: Weather condition ('CLEAR', 'CLOUDY', 'RAIN', 'SNOW')\n",
        "        temperature: Temperature in Fahrenheit\n",
        "        day_of_week: Day of week (0=Monday, 6=Sunday)\n",
        "        month: Month (1-12)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (bike_count_prediction, accident_severity_prediction)\n",
        "    \"\"\"\n",
        "    # Encode inputs\n",
        "    intersection_encoded = intersection_encoder.transform([intersection])[0]\n",
        "    weather_encoded = weather_encoder.transform([weather])[0]\n",
        "    \n",
        "    # Create time features\n",
        "    hour = time_15min // 4\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "    day_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
        "    day_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
        "    month_sin = np.sin(2 * np.pi * month / 12)\n",
        "    month_cos = np.cos(2 * np.pi * month / 12)\n",
        "    \n",
        "    # Normalize temperature\n",
        "    temp_normalized = temp_scaler.transform([[temperature]])[0][0]\n",
        "    \n",
        "    # Create feature vector\n",
        "    features = np.array([[\n",
        "        time_15min, intersection_encoded, weather_encoded,\n",
        "        hour_sin, hour_cos, day_sin, day_cos, month_sin, month_cos,\n",
        "        temp_normalized\n",
        "    ]])\n",
        "    \n",
        "    # Scale features\n",
        "    features_scaled = feature_scaler.transform(features)\n",
        "    \n",
        "    # Make prediction\n",
        "    predictions = model.predict(features_scaled, verbose=0)\n",
        "    bike_count = predictions[0][0][0]\n",
        "    accident_severity = predictions[1][0][0]\n",
        "    \n",
        "    return bike_count, accident_severity\n",
        "\n",
        "# Test the prediction function\n",
        "print(\"\\\\n=== TESTING PREDICTION FUNCTION ===\")\n",
        "test_prediction = predict_bicycle_traffic(\n",
        "    intersection='BROADWAY',\n",
        "    time_15min=32,  # 8:00 AM\n",
        "    weather='CLEAR',\n",
        "    temperature=70,\n",
        "    day_of_week=0,  # Monday\n",
        "    month=6  # June\n",
        ")\n",
        "\n",
        "print(f\"Test prediction for Broadway at 8:00 AM on Monday in June:\")\n",
        "print(f\"  Predicted bike count: {test_prediction[0]:.1f}\")\n",
        "print(f\"  Predicted accident severity: {test_prediction[1]:.3f}\")\n",
        "\n",
        "print(\"\\\\nModel training and setup completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
